{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Audio Explorer docs site! Audio Explorer helps in audio data discovery and labelling by utilising unsupervised and supervised machine learning - and good deal of statistics with digital signal processing.","title":"Home"},{"location":"#welcome-to-audio-explorer-docs-site","text":"Audio Explorer helps in audio data discovery and labelling by utilising unsupervised and supervised machine learning - and good deal of statistics with digital signal processing.","title":"Welcome to Audio Explorer docs site!"},{"location":"about/","text":"About Hi, I'm Lukasz! I am a software engineer and scientist. Why Audio Explorer? Manual labelling of audio is time consuming and error prone. With this tool we aim to augment user by allowing to easily navigate recordings and label selected audio pieces. Instead of looking at raw audio, we extract number of audio features from each sample. The latter typically consists of dozens of calculated values (features), which would be impossible to visualise (e.g. 20 features per sample effectively means 20-dimensional space). Audio Explorer allows to compute over 100 features per audio fragment. How do we achieve that? The program computes audio features per short fragments of submitted audio piece and then finds projection to 2-dimensional space by using linear or non-linear dimensionality reduction . Audio fragments are then represented as points; similar sample will be close together, while those which are different further apart. Acknowledgement My special thanks to: AWS Cloud Credits for Research for supporting the project! Thanks to AWS I could rapidly prototype the models and the app itself. My colleagues from RSPB who have supplied the audio recordings and supported along the way. Cheers!","title":"About"},{"location":"about/#about","text":"Hi, I'm Lukasz! I am a software engineer and scientist.","title":"About"},{"location":"about/#why-audio-explorer","text":"Manual labelling of audio is time consuming and error prone. With this tool we aim to augment user by allowing to easily navigate recordings and label selected audio pieces. Instead of looking at raw audio, we extract number of audio features from each sample. The latter typically consists of dozens of calculated values (features), which would be impossible to visualise (e.g. 20 features per sample effectively means 20-dimensional space). Audio Explorer allows to compute over 100 features per audio fragment.","title":"Why Audio Explorer?"},{"location":"about/#how-do-we-achieve-that","text":"The program computes audio features per short fragments of submitted audio piece and then finds projection to 2-dimensional space by using linear or non-linear dimensionality reduction . Audio fragments are then represented as points; similar sample will be close together, while those which are different further apart.","title":"How do we achieve that?"},{"location":"about/#acknowledgement","text":"My special thanks to: AWS Cloud Credits for Research for supporting the project! Thanks to AWS I could rapidly prototype the models and the app itself. My colleagues from RSPB who have supplied the audio recordings and supported along the way. Cheers!","title":"Acknowledgement"},{"location":"app_description/","text":"Audio Explorer Copyright 2019 Lukasz Tracewski Audio Explorer helps in audio data discovery and labelling by utilising unsupervised and supervised machine learning - and good deal of statistics with digital signal processing. Documentation Check the docs to learn about the program. Problem reports and feedback Please post an issue on GitHub project page .","title":"App description"},{"location":"app_description/#audio-explorer","text":"Copyright 2019 Lukasz Tracewski Audio Explorer helps in audio data discovery and labelling by utilising unsupervised and supervised machine learning - and good deal of statistics with digital signal processing.","title":"Audio Explorer"},{"location":"app_description/#documentation","text":"Check the docs to learn about the program.","title":"Documentation"},{"location":"app_description/#problem-reports-and-feedback","text":"Please post an issue on GitHub project page .","title":"Problem reports and feedback"},{"location":"audio_embedding/","text":"Audio embedding Audio features Given audio fragment, thee following features are computed: Frequency statistics: mean, median, first, third and inter quartile Pitch statistics: mean, median, first, third and inter quartile Chroma : short-term pitch profile Linear Predictor Coefficients ( LPC ) Line Spectral Frequency ( LSF ) coefficients Mel-Frequency Cepstral Coefficients ( MFCC ) Octave Band Signal Intensity (OBSI) Spectral crest factor per band Decrease: average spectral slope Flatness: spectral flatness using the ratio between geometric and arithmetic mean Flux: flux of spectrum between consecutives frames Rolloff: frequency so that 99% of the energy is contained below Variation: normalized correlation of spectrum between consecutive frames Dimensionality reduction Dimensionality reduction techniques reduce number of variables by projecting them to a lower-dimensional space. The aim in our case is to retain as much as possible of original information, while enjoying exploration of the data in much in familiar 2D space. We're looking at following methods: Uniform Manifold Approximation and Projection t-Distributed Stochastic Neighbor embedding Principal Component Analysis Kernel Principal Component Analysis Factor Analysis Independent Component Analysis Isomap: Isometric Mapping Spectral embedding Locally Linear Embedding","title":"Audio embedding"},{"location":"audio_embedding/#audio-embedding","text":"","title":"Audio embedding"},{"location":"audio_embedding/#audio-features","text":"Given audio fragment, thee following features are computed: Frequency statistics: mean, median, first, third and inter quartile Pitch statistics: mean, median, first, third and inter quartile Chroma : short-term pitch profile Linear Predictor Coefficients ( LPC ) Line Spectral Frequency ( LSF ) coefficients Mel-Frequency Cepstral Coefficients ( MFCC ) Octave Band Signal Intensity (OBSI) Spectral crest factor per band Decrease: average spectral slope Flatness: spectral flatness using the ratio between geometric and arithmetic mean Flux: flux of spectrum between consecutives frames Rolloff: frequency so that 99% of the energy is contained below Variation: normalized correlation of spectrum between consecutive frames","title":"Audio features"},{"location":"audio_embedding/#dimensionality-reduction","text":"Dimensionality reduction techniques reduce number of variables by projecting them to a lower-dimensional space. The aim in our case is to retain as much as possible of original information, while enjoying exploration of the data in much in familiar 2D space. We're looking at following methods: Uniform Manifold Approximation and Projection t-Distributed Stochastic Neighbor embedding Principal Component Analysis Kernel Principal Component Analysis Factor Analysis Independent Component Analysis Isomap: Isometric Mapping Spectral embedding Locally Linear Embedding","title":"Dimensionality reduction"},{"location":"case_storm_petrels/","text":"Case study: Storm Petrels Introduction Case of Storm Petrels from St Helena was the main drive behind creating the Audio Explorer. RSPB has lots of audio recordings from the island and was interested in estimating population change and their activity. Typically, supervised machine learning systems need reliable labels to inform the algorithm about what is that we are looking for. Labelling of audio recordings is labourious, error prone and subjective activity. It depends on our perception and judgement. One person might bird call that other would consider too faint - or not hear at all. Garbage In - Garbage Out The trouble with that approach is that when incorrect labels are fed to supervised machine learning algorithms, we risk that the system won't learn what we intend - and only a narrow band that is presented. Worse, if we skip over some calls and denote them as noise, the system might get quite confused on what's the species of interest. That's what was happening in case of storm petrels and labels we have had initially. We needed better labels, not only to tell the system how the storm petrel sounds - but also how it does not sound. Data The recordings from St Helena consist of over 500h of data recorded in stereo with 44khz sampling rate. We downsampled and compressed the data such that upload to Audio Explorer would take much less time - check the FAQ for details. For the analysis we used 16khz sampling rate. Method Our goal was to count all storm petrel calls. To achieve it Having quality labels is crucial to enable successfuly supervised machine learning,","title":"Case study: Storm Petrels"},{"location":"case_storm_petrels/#case-study-storm-petrels","text":"","title":"Case study: Storm Petrels"},{"location":"case_storm_petrels/#introduction","text":"Case of Storm Petrels from St Helena was the main drive behind creating the Audio Explorer. RSPB has lots of audio recordings from the island and was interested in estimating population change and their activity. Typically, supervised machine learning systems need reliable labels to inform the algorithm about what is that we are looking for. Labelling of audio recordings is labourious, error prone and subjective activity. It depends on our perception and judgement. One person might bird call that other would consider too faint - or not hear at all.","title":"Introduction"},{"location":"case_storm_petrels/#garbage-in-garbage-out","text":"The trouble with that approach is that when incorrect labels are fed to supervised machine learning algorithms, we risk that the system won't learn what we intend - and only a narrow band that is presented. Worse, if we skip over some calls and denote them as noise, the system might get quite confused on what's the species of interest. That's what was happening in case of storm petrels and labels we have had initially. We needed better labels, not only to tell the system how the storm petrel sounds - but also how it does not sound.","title":"Garbage In - Garbage Out"},{"location":"case_storm_petrels/#data","text":"The recordings from St Helena consist of over 500h of data recorded in stereo with 44khz sampling rate. We downsampled and compressed the data such that upload to Audio Explorer would take much less time - check the FAQ for details. For the analysis we used 16khz sampling rate.","title":"Data"},{"location":"case_storm_petrels/#method","text":"Our goal was to count all storm petrel calls. To achieve it Having quality labels is crucial to enable successfuly supervised machine learning,","title":"Method"},{"location":"command_line/","text":"Command Line Interface audiocli is a command line program that helps in extracting audio features and diemnsionality reduction. It's primary purpose is to build offline embeddings for the Audio Explorer. User can create a model with large volume of audio data and then use it to embed new audio files into that space. Usage: audiocli.py [OPTIONS] COMMAND [ARGS]... Options: --quiet Run in a silent mode --help Show this message and exit. Commands: a2f Audio to HDF5 features f2m Features to embedding model m2e Model to embedddings Following options are available: a2f - Audio to Features Usage: audiocli.py a2f [OPTIONS] Audio to HDF5 features Options: -in, --input TEXT Path to audio in WAV format. [required] -out, --output TEXT Output file or directory. If directory does not exist it will be created. The output files will have the same base name as input. -j, --jobs INTEGER Number of jobs to run. Defaults to all cores [default: -1] -c, --config PATH Feature extractor config. -m, --multi Process audio files in parallel. The setting will produce an HDF5 file per input, with the same base name. Large memory footprint. If not set, a single output file will be produced. -f, --format [fixed|table] HDF5 format. Table is slightly slower and requires pytables (will not work outside Python), but allows to read specific columns. --help Show this message and exit. Example: ./audiocli.py a2f --input data/raw/storm_petrels_16k/ --output data/features/features_02s/ --jobs 4 --config audioexplorer/algo_config.ini --multi --format table The program loads complete file into memory, so watch out for memory usage f2m - Features to Model Usage: audiocli.py f2m [OPTIONS] Features to embedding model Options: -in, --input TEXT Path to h5 features. [required] -out, --output TEXT Output directory. -j, --jobs INTEGER Number of jobs to run [default: -1] -a, --algo [umap|tsne|isomap|spectral|loclin|pca|kpca|fa|ica] Embedding to use -p, --grid PATH JSON with grid search parameters for the embedding algo --help Show this message and exit. Example: audiocli.py f2m --input data/features/features_02s/ --output data/models/ --jobs 6 --algo umap --grid data/umap_grid.json --select freq","title":"Command Line Interface"},{"location":"command_line/#command-line-interface","text":"audiocli is a command line program that helps in extracting audio features and diemnsionality reduction. It's primary purpose is to build offline embeddings for the Audio Explorer. User can create a model with large volume of audio data and then use it to embed new audio files into that space. Usage: audiocli.py [OPTIONS] COMMAND [ARGS]... Options: --quiet Run in a silent mode --help Show this message and exit. Commands: a2f Audio to HDF5 features f2m Features to embedding model m2e Model to embedddings Following options are available:","title":"Command Line Interface"},{"location":"command_line/#a2f-audio-to-features","text":"Usage: audiocli.py a2f [OPTIONS] Audio to HDF5 features Options: -in, --input TEXT Path to audio in WAV format. [required] -out, --output TEXT Output file or directory. If directory does not exist it will be created. The output files will have the same base name as input. -j, --jobs INTEGER Number of jobs to run. Defaults to all cores [default: -1] -c, --config PATH Feature extractor config. -m, --multi Process audio files in parallel. The setting will produce an HDF5 file per input, with the same base name. Large memory footprint. If not set, a single output file will be produced. -f, --format [fixed|table] HDF5 format. Table is slightly slower and requires pytables (will not work outside Python), but allows to read specific columns. --help Show this message and exit. Example: ./audiocli.py a2f --input data/raw/storm_petrels_16k/ --output data/features/features_02s/ --jobs 4 --config audioexplorer/algo_config.ini --multi --format table The program loads complete file into memory, so watch out for memory usage","title":"a2f - Audio to Features"},{"location":"command_line/#f2m-features-to-model","text":"Usage: audiocli.py f2m [OPTIONS] Features to embedding model Options: -in, --input TEXT Path to h5 features. [required] -out, --output TEXT Output directory. -j, --jobs INTEGER Number of jobs to run [default: -1] -a, --algo [umap|tsne|isomap|spectral|loclin|pca|kpca|fa|ica] Embedding to use -p, --grid PATH JSON with grid search parameters for the embedding algo --help Show this message and exit. Example: audiocli.py f2m --input data/features/features_02s/ --output data/models/ --jobs 6 --algo umap --grid data/umap_grid.json --select freq","title":"f2m - Features to Model"},{"location":"developer_notes/","text":"Developer notes The web app is hosted on AWS and uses following components: Elastic Beanstalk with Application Load Balancer: orchestration S3: storage Route 53: DNS service Relational Database Service (RDS) with postgresql Secrets Manager: protect secrets (plotly, ipinfo) Gunicorn: WSGI Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Set up development environment To set up your development environment, run the following commands: Fork and clone the Audio Explorer repo . Move into the clone: cd audio-explorer . Create Anaconda environment: conda env create -f environment.yml . Documentation Build docs: mkdocs build Deploy docs to GitHub: mkdocs gh-deploy Deployment Elastic Beanstalk Install AWS CLI for EB: pip install awsebcli Prcedure: Initialise the environment: eb init Create and deploy new instance of type: eb create -i [type] Deploy eb deploy . Hit it to deliver every new chunk. SSH to the machine on Elastic Beanstalk: eb ssh Docker Build docker image: docker image build [path] user/name:latest Example: docker build -t tracek/audio-explorer:latest -t tracek/audio-explorer:0.1 . SSH to the container: sudo docker exec -it [container id] /bin/bash Troubleshooting Problem: Elastic Beanstalk deployment via Docker fails due thin pool getting full. Solution: Use EC2 that has a drive with sufficient space (mind most of EC2 uses EBS volumes) Follow instructions from Server Fault The latter comes recommended, as default thin pool is ~12 GB, which is not enough for Audio Explorer. Bumping it to 40 GB is a good choice - that's why we have .ebextensions/04_docker.config . Pull Request Guidelines Use the GitHub flow when proposing contributions to this repository (i.e. create a feature branch and submit a PR against the master branch). How to create pull request is discussed here .","title":"Developer notes"},{"location":"developer_notes/#developer-notes","text":"The web app is hosted on AWS and uses following components: Elastic Beanstalk with Application Load Balancer: orchestration S3: storage Route 53: DNS service Relational Database Service (RDS) with postgresql Secrets Manager: protect secrets (plotly, ipinfo) Gunicorn: WSGI Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.","title":"Developer notes"},{"location":"developer_notes/#set-up-development-environment","text":"To set up your development environment, run the following commands: Fork and clone the Audio Explorer repo . Move into the clone: cd audio-explorer . Create Anaconda environment: conda env create -f environment.yml .","title":"Set up development environment"},{"location":"developer_notes/#documentation","text":"Build docs: mkdocs build Deploy docs to GitHub: mkdocs gh-deploy","title":"Documentation"},{"location":"developer_notes/#deployment","text":"","title":"Deployment"},{"location":"developer_notes/#elastic-beanstalk","text":"Install AWS CLI for EB: pip install awsebcli Prcedure: Initialise the environment: eb init Create and deploy new instance of type: eb create -i [type] Deploy eb deploy . Hit it to deliver every new chunk. SSH to the machine on Elastic Beanstalk: eb ssh","title":"Elastic Beanstalk"},{"location":"developer_notes/#docker","text":"Build docker image: docker image build [path] user/name:latest Example: docker build -t tracek/audio-explorer:latest -t tracek/audio-explorer:0.1 . SSH to the container: sudo docker exec -it [container id] /bin/bash","title":"Docker"},{"location":"developer_notes/#troubleshooting","text":"Problem: Elastic Beanstalk deployment via Docker fails due thin pool getting full. Solution: Use EC2 that has a drive with sufficient space (mind most of EC2 uses EBS volumes) Follow instructions from Server Fault The latter comes recommended, as default thin pool is ~12 GB, which is not enough for Audio Explorer. Bumping it to 40 GB is a good choice - that's why we have .ebextensions/04_docker.config .","title":"Troubleshooting"},{"location":"developer_notes/#pull-request-guidelines","text":"Use the GitHub flow when proposing contributions to this repository (i.e. create a feature branch and submit a PR against the master branch). How to create pull request is discussed here .","title":"Pull Request Guidelines"},{"location":"faq/","text":"FAQ Why bandpass filter only goes to 8000 Hz? The audio you upload is converted to mono-channel 16 kHz which translates to 8 kHz (half of the sampling frequency). Read on Nyquist rate for explanation. My files take long to upload Consider reducing the sampling frequency to 16kHz, number of channels to 1 and compressing the file to e.g. mp3. There is an excellent cross-platform tool called SoX that can help you. If you have a lot of files that you want to process, here's how you can process them in parallel on Linux / Mac: find . -name '*.wav' -type f -print0 | parallel -0 sox --norm {} -r 16000 --channels 1 your-path/{.}.mp3 The command: * Finds all wav * Pipes then to parallel tool that will execute subsequent call in parallel * SoX normalises the audio, resamples to 16 kHz and one channel and then converts to mp3 Read parallel cheat sheet and full manual for details. How to install the software? There are a few approaches: * Start from scratch from the repo * Use Docker image * Check with me about producing a Virtual Machine image with the software. We could use e.g. VirtualBox .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#why-bandpass-filter-only-goes-to-8000-hz","text":"The audio you upload is converted to mono-channel 16 kHz which translates to 8 kHz (half of the sampling frequency). Read on Nyquist rate for explanation.","title":"Why bandpass filter only goes to 8000 Hz?"},{"location":"faq/#my-files-take-long-to-upload","text":"Consider reducing the sampling frequency to 16kHz, number of channels to 1 and compressing the file to e.g. mp3. There is an excellent cross-platform tool called SoX that can help you. If you have a lot of files that you want to process, here's how you can process them in parallel on Linux / Mac: find . -name '*.wav' -type f -print0 | parallel -0 sox --norm {} -r 16000 --channels 1 your-path/{.}.mp3 The command: * Finds all wav * Pipes then to parallel tool that will execute subsequent call in parallel * SoX normalises the audio, resamples to 16 kHz and one channel and then converts to mp3 Read parallel cheat sheet and full manual for details.","title":"My files take long to upload"},{"location":"faq/#how-to-install-the-software","text":"There are a few approaches: * Start from scratch from the repo * Use Docker image * Check with me about producing a Virtual Machine image with the software. We could use e.g. VirtualBox .","title":"How to install the software?"},{"location":"getting_started/","text":"Getting started With the Audio Explorer you will be able to easily explore and label sounds present in your recordings. In this section we are going to walk through all options of the app, explain what they do and how they can help you in achieving the task. Explore When you start the app, you arrive in Explore section. That's where you can upload the audio, tune various parameters, clean audio and plot the results. The latter will consist of 2-d embedding and spectrogram. Spectrogram is a plot of frequency and its amplitude against time. Embedding is the main functionality of the app that sets it apart from most of such software in the market. It allows us to create a 2d scatter plot of sounds by computing various sound features (typically 6 to over 100) and then reducing the dimensionality to 2d. It's done in such a way that most of the information from the high-dimensional space is retained. If we get it right, similar sounds will be close together while different further apart. Preparation To upload a recording, use Upload button. Currently only a single recording can be uploaded at a time, but it's possible to extend the functionality (please drop me a feature request via GitHub). Before you hit the button, consider checking and adjusting settings. The settings are applied every time you hit Upload or Apply button. The difference is that while Upload lets you select a file from your file system, Apply works on currently uploaded file. Settings Your settings will vary between applications and use cases, there's no one that would fit all problems, which is why Audio Explorer gives user freedom in selecting the right one. It's a good idea to start with default ones. Embedding type and features The available algorithms and features are discussed in Audio Embedding section. Uniform Manifold Approximation and Projection comes highly recommended. Selection of features is case-dependent. It's a good idea to start with pitch , which calculates mean, median, first and third quantile, interquantile, min and max for each audio fragment (pitch is calculated per window equal to number of samples set in FFT setting). It uses excellent algorithm Yinfft developed by Paul Brossier (more here ). FFT FFT stand for Fast Fourier Transform, in this case window size (in number of samples) to be used by the algorithm. Bandpass filter Bandpass filter allows user to reduce frequencies below / above certain threshold. The filters roll off is at 6dB per octave (20dB per decade). Onset detection threshold This method computes the High Frequency Content (HFC) of the input spectral frame from aubio . If disabled (set to None ), the program will simply chop the recording into pieces of length determined by Sample length setting. The threshold ranges from 0.005 (very sensitive, picks up almost everything) to 0.1 , where only stronger signals are surfaced from the background. Sample length Length of the audio fragment to consider. It's currently set to a fixed length, but could be made dynamic if needed (to find onset and offset of the sound). Number of neighbours - Important! Parameter specific to the Uniform Manifold Approximation and Projection algorithm. In general, the lower the number, more local structures can be resolved, at expense of good global separation. If we take large number of neighbours, different sounds will be far apart, while similar clustered very closely together. Mind that number of neighbours for the algorithm to consider should be (much) lower than number of audio fragments that we are analysing. Should it happen that there are very few onsets in the recording (say, 8), the algorithm will fail to produce embedding. Consider lowering the value to minimum in such cases or lowering the onset detection threshold. Supported audio formats Audio Explorer supports majority of popular audio formats thanks to excellent SoX software and works with both mono and stereo. Here's a comprehensive list: 8svx aif aifc aiff aiffc al amb amr-nb amr-wb anb au avr awb caf cdda cdr cvs cvsd cvu dat dvms f32 f4 f64 f8 fap flac fssd gsm gsrt hcom htk ima ircam la lpc lpc10 lu mat mat4 mat5 maud mp2 mp3 nist ogg paf prc pvf raw s1 s16 s2 s24 s3 s32 s4 s8 sb sd2 sds sf sl sln smp snd sndfile sndr sndt sou sox sph sw txw u1 u16 u2 u24 u3 u32 u4 u8 ub ul uw vms voc vorbis vox w64 wav wavpcm wv wve xa xi Uploaded recording will be normalised and converted to single-channel 16 khz WAV. Upload Once you hit Upload , your recording will be uploaded, chopped into fragments and audio features calculated per fragment. In the last step an embedding will be produced like this one: What am I looking at? The scatter plot is the result of running the dimensionality reduction algorithms on audio recordings resulting in a 2D visualization of the dataset. Each data point is a short sample retrieved from audio. You can click on each point and play an audio associated with the given point. In the left bottom corner you will find a name of your file and option to download (selected) features associated with your audio. Spectral plot Once file is uploaded, spectrum of the audio will be plotted in lower right corner. Spectrum Spectrogram Select number of points and spectrum will be plotted only for the given points. Click on a single point and a spectrogram will be plotted. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. Spectrogram allows further verification of similarity between samples as well as provide insights into frequency structure of the signal. Plotted spectrogram has an extra margin of 0.05 second from both ends to enable better inspection. The extra margin is not considered during the analysis and is visible as a thin black line. Tip If your display resolution is fairly small, I'd recommend to zoom out the browser for better experience. Selection Once you hover your mouse pointer over the graph, an extra set of icons will appear in upper right corner of the graph. They allow you to better navigate the graph, zoom in and, most interestingly, select points with Lasso tool . Why would you want to select specific points: Label signal of interest ( Download button next to the file name) Remove selected frequencies ( Remove selected frequencies button below the Spectral plot ) Inspect when in time these appear (available via Profile tab) Inspect the features (available via Table tab) Download The Download button allows to download features from the selected audio fragments to your local file system. Edit box to the left from the download link allows to change the file name. Play audio You can play audio fragment by clicking on a point on the main scatter plot. Mind that what you hear and see is longer than selected Sample length by 0.05s. from both ends. Margin is added to the beginning and end to get better impression of the sound surrounding the fragment. Didn't get what you expected? Adjust the settings and hit Apply . Profile That's where you can see a spectrogram of the complete audio. If you mark any points on the main graph, these will be highlighted on the spectrogram in light red. You can zoom in the graph by holding left mouse button and dragging selection over area of interest. Table In this section you can inspect all features as a table. Features can be sorted and filtered. You can use the following expressions: <= , < , >= and > , e.g. > 2000 . Worflow example Tune the algorithm parameters or accept the defaults. Upload an audio file you want to analyse. If you don't have anything at hand, here is an audio that contains some bird calls, primarily storm petrels, recorded on St. Helena. Can you spot bird calls on the scatter plot? To download the recording, right-click on the link and select \"save link as...\" and then Upload it. Play with the parameters, add / remove features and see how it influences the plot by clicking Apply . Click a point on the graph to hear the audio and see the spectrogram. Calculated audio features can be inspected, sorted and filtered through custom-made query language by selecting Table tab. The selection will be reflected in Table . Use Lasso select (top right menu that appears after hovering over the graph) to select interesting cluster. For the selected audio fragments a power spectrum will be plotted (units: Voltage 2 ), scaled to dB. Now that we have frequencies present in the selected fragments, user can decide to reduce presence of these frequencies in rest of the audio. Once you're happy with the selection, you can download the data from Table . Proper analysis is described in the Use cases section.","title":"Getting started"},{"location":"getting_started/#getting-started","text":"With the Audio Explorer you will be able to easily explore and label sounds present in your recordings. In this section we are going to walk through all options of the app, explain what they do and how they can help you in achieving the task.","title":"Getting started"},{"location":"getting_started/#explore","text":"When you start the app, you arrive in Explore section. That's where you can upload the audio, tune various parameters, clean audio and plot the results. The latter will consist of 2-d embedding and spectrogram. Spectrogram is a plot of frequency and its amplitude against time. Embedding is the main functionality of the app that sets it apart from most of such software in the market. It allows us to create a 2d scatter plot of sounds by computing various sound features (typically 6 to over 100) and then reducing the dimensionality to 2d. It's done in such a way that most of the information from the high-dimensional space is retained. If we get it right, similar sounds will be close together while different further apart.","title":"Explore"},{"location":"getting_started/#preparation","text":"To upload a recording, use Upload button. Currently only a single recording can be uploaded at a time, but it's possible to extend the functionality (please drop me a feature request via GitHub). Before you hit the button, consider checking and adjusting settings. The settings are applied every time you hit Upload or Apply button. The difference is that while Upload lets you select a file from your file system, Apply works on currently uploaded file.","title":"Preparation"},{"location":"getting_started/#settings","text":"Your settings will vary between applications and use cases, there's no one that would fit all problems, which is why Audio Explorer gives user freedom in selecting the right one. It's a good idea to start with default ones.","title":"Settings"},{"location":"getting_started/#embedding-type-and-features","text":"The available algorithms and features are discussed in Audio Embedding section. Uniform Manifold Approximation and Projection comes highly recommended. Selection of features is case-dependent. It's a good idea to start with pitch , which calculates mean, median, first and third quantile, interquantile, min and max for each audio fragment (pitch is calculated per window equal to number of samples set in FFT setting). It uses excellent algorithm Yinfft developed by Paul Brossier (more here ).","title":"Embedding type and features"},{"location":"getting_started/#fft","text":"FFT stand for Fast Fourier Transform, in this case window size (in number of samples) to be used by the algorithm.","title":"FFT"},{"location":"getting_started/#bandpass-filter","text":"Bandpass filter allows user to reduce frequencies below / above certain threshold. The filters roll off is at 6dB per octave (20dB per decade).","title":"Bandpass filter"},{"location":"getting_started/#onset-detection-threshold","text":"This method computes the High Frequency Content (HFC) of the input spectral frame from aubio . If disabled (set to None ), the program will simply chop the recording into pieces of length determined by Sample length setting. The threshold ranges from 0.005 (very sensitive, picks up almost everything) to 0.1 , where only stronger signals are surfaced from the background.","title":"Onset detection threshold"},{"location":"getting_started/#sample-length","text":"Length of the audio fragment to consider. It's currently set to a fixed length, but could be made dynamic if needed (to find onset and offset of the sound).","title":"Sample length"},{"location":"getting_started/#number-of-neighbours-important","text":"Parameter specific to the Uniform Manifold Approximation and Projection algorithm. In general, the lower the number, more local structures can be resolved, at expense of good global separation. If we take large number of neighbours, different sounds will be far apart, while similar clustered very closely together. Mind that number of neighbours for the algorithm to consider should be (much) lower than number of audio fragments that we are analysing. Should it happen that there are very few onsets in the recording (say, 8), the algorithm will fail to produce embedding. Consider lowering the value to minimum in such cases or lowering the onset detection threshold.","title":"Number of neighbours - Important!"},{"location":"getting_started/#supported-audio-formats","text":"Audio Explorer supports majority of popular audio formats thanks to excellent SoX software and works with both mono and stereo. Here's a comprehensive list: 8svx aif aifc aiff aiffc al amb amr-nb amr-wb anb au avr awb caf cdda cdr cvs cvsd cvu dat dvms f32 f4 f64 f8 fap flac fssd gsm gsrt hcom htk ima ircam la lpc lpc10 lu mat mat4 mat5 maud mp2 mp3 nist ogg paf prc pvf raw s1 s16 s2 s24 s3 s32 s4 s8 sb sd2 sds sf sl sln smp snd sndfile sndr sndt sou sox sph sw txw u1 u16 u2 u24 u3 u32 u4 u8 ub ul uw vms voc vorbis vox w64 wav wavpcm wv wve xa xi Uploaded recording will be normalised and converted to single-channel 16 khz WAV.","title":"Supported audio formats"},{"location":"getting_started/#upload","text":"Once you hit Upload , your recording will be uploaded, chopped into fragments and audio features calculated per fragment. In the last step an embedding will be produced like this one:","title":"Upload"},{"location":"getting_started/#what-am-i-looking-at","text":"The scatter plot is the result of running the dimensionality reduction algorithms on audio recordings resulting in a 2D visualization of the dataset. Each data point is a short sample retrieved from audio. You can click on each point and play an audio associated with the given point. In the left bottom corner you will find a name of your file and option to download (selected) features associated with your audio.","title":"What am I looking at?"},{"location":"getting_started/#spectral-plot","text":"Once file is uploaded, spectrum of the audio will be plotted in lower right corner. Spectrum Spectrogram Select number of points and spectrum will be plotted only for the given points. Click on a single point and a spectrogram will be plotted. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. Spectrogram allows further verification of similarity between samples as well as provide insights into frequency structure of the signal. Plotted spectrogram has an extra margin of 0.05 second from both ends to enable better inspection. The extra margin is not considered during the analysis and is visible as a thin black line.","title":"Spectral plot"},{"location":"getting_started/#tip","text":"If your display resolution is fairly small, I'd recommend to zoom out the browser for better experience.","title":"Tip"},{"location":"getting_started/#selection","text":"Once you hover your mouse pointer over the graph, an extra set of icons will appear in upper right corner of the graph. They allow you to better navigate the graph, zoom in and, most interestingly, select points with Lasso tool . Why would you want to select specific points: Label signal of interest ( Download button next to the file name) Remove selected frequencies ( Remove selected frequencies button below the Spectral plot ) Inspect when in time these appear (available via Profile tab) Inspect the features (available via Table tab)","title":"Selection"},{"location":"getting_started/#download","text":"The Download button allows to download features from the selected audio fragments to your local file system. Edit box to the left from the download link allows to change the file name.","title":"Download"},{"location":"getting_started/#play-audio","text":"You can play audio fragment by clicking on a point on the main scatter plot. Mind that what you hear and see is longer than selected Sample length by 0.05s. from both ends. Margin is added to the beginning and end to get better impression of the sound surrounding the fragment.","title":"Play audio"},{"location":"getting_started/#didnt-get-what-you-expected","text":"Adjust the settings and hit Apply .","title":"Didn't get what you expected?"},{"location":"getting_started/#profile","text":"That's where you can see a spectrogram of the complete audio. If you mark any points on the main graph, these will be highlighted on the spectrogram in light red. You can zoom in the graph by holding left mouse button and dragging selection over area of interest.","title":"Profile"},{"location":"getting_started/#table","text":"In this section you can inspect all features as a table. Features can be sorted and filtered. You can use the following expressions: <= , < , >= and > , e.g. > 2000 .","title":"Table"},{"location":"getting_started/#worflow-example","text":"Tune the algorithm parameters or accept the defaults. Upload an audio file you want to analyse. If you don't have anything at hand, here is an audio that contains some bird calls, primarily storm petrels, recorded on St. Helena. Can you spot bird calls on the scatter plot? To download the recording, right-click on the link and select \"save link as...\" and then Upload it. Play with the parameters, add / remove features and see how it influences the plot by clicking Apply . Click a point on the graph to hear the audio and see the spectrogram. Calculated audio features can be inspected, sorted and filtered through custom-made query language by selecting Table tab. The selection will be reflected in Table . Use Lasso select (top right menu that appears after hovering over the graph) to select interesting cluster. For the selected audio fragments a power spectrum will be plotted (units: Voltage 2 ), scaled to dB. Now that we have frequencies present in the selected fragments, user can decide to reduce presence of these frequencies in rest of the audio. Once you're happy with the selection, you can download the data from Table . Proper analysis is described in the Use cases section.","title":"Worflow example"}]}